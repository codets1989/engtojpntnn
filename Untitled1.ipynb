{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a016120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "from typing import Tuple\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d558ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jpn.txt',encoding='UTF-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs=[]\n",
    "for line in lines:\n",
    "        en,jp=line.split(\"\\t\",1)\n",
    "        jp = re.sub('\\t' , \"\",jp)\n",
    "        jp = re.sub('\\W+' , \"\",jp)\n",
    "        en = re.sub('[%s]' % re.escape(string.punctuation) , \"\",en)\n",
    "        jp = \"[start]\" + jp + \"[end]\"\n",
    "        text_pairs.append((en,jp))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ed3cf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Snow has been falling steadily since this morning', '[start]朝から休みなく雪が降り続いている[end]')\n",
      "('Tom soon adapted himself to school life', '[start]トムはすぐに学校に馴染んだ[end]')\n",
      "('We dont talk a lot', '[start]そんなに話しないよ[end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(3):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc17b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "totaldatalen = len(text_pairs)\n",
    "num_val_samples = int(0.15*len(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17cd5f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = totaldatalen - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:train_samples]\n",
    "val_pairs= text_pairs[train_samples:train_samples+ num_val_samples]\n",
    "test_pairs= text_pairs[train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "754f6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec94da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "def custom_standarization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return lowercase\n",
    "vocab_size = 15000\n",
    "sequence_length=20\n",
    "source_vectorization= layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length,\n",
    "    standardize = custom_standarization,\n",
    ")\n",
    "target_vectorization= layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length+1,\n",
    "    split = 'character'  \n",
    ")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75741da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_jp_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_jp_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d45a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 64\n",
    "def format_dataset(eng,jp):\n",
    "    eng = source_vectorization(eng)\n",
    "    jp = target_vectorization(jp)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"japanese\":jp[:,:-1],\n",
    "    },jp[:,1:])\n",
    "def make_dataset(pairs):\n",
    "    eng_texts , jp_texts = zip(*pairs)\n",
    "    eng_texts = list (eng_texts)\n",
    "    jp_texts = list(jp_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts,jp_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset,num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4277a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "{TensorShape([64, 20])}\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print ({targets.shape})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1b4f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a051fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "latent_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "448044ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self,embed_size,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads= num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_size)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim,activation=\"relu\"),layers.Dense(embed_size),])\n",
    "        self.layernorm_1= layers.LayerNormalization()\n",
    "        self.layernorm_2= layers.LayerNormalization()\n",
    "    def call(self,inputs,mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:,tf.newaxis,:]\n",
    "        attention_output= self.attention(inputs,inputs,attention_mask=mask)\n",
    "        proj_input= self.layernorm_1(inputs+ attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return  self.layernorm_2(proj_input + proj_output)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_size\": self.embed_size,\n",
    "            \"num_heads\":  self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea5e5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self,sequence_length,input_dim,output_dim,**kwargs):\n",
    "                    super().__init__(**kwargs)\n",
    "                    self.token_embeddings = layers.Embedding(\n",
    "                    input_dim=input_dim,output_dim=output_dim)\n",
    "                    self.position_embeddings = layers.Embedding(\n",
    "                    input_dim=sequence_length,output_dim=output_dim)\n",
    "                    self.sequence_length=sequence_length\n",
    "                    self.output_dim=output_dim\n",
    "    def call(self,inputs):\n",
    "            length= tf.shape(inputs)[-1]\n",
    "            positions = tf.range(start=0,limit=length,delta=1)\n",
    "            embedded_tokens = self.token_embeddings(inputs)\n",
    "            embedded_positions = self.position_embeddings(positions)\n",
    "            return embedded_tokens + embedded_positions\n",
    "    def compute_mask(self,inputs,mask=None):\n",
    "            return tf.math.not_equal(inputs,0)\n",
    "    def get_config(self):\n",
    "            config = super(PositionalEmbedding,self).get_config()\n",
    "            config.update({\n",
    "                       \"output_dim\":self.output_dim,\n",
    "                       \"sequence_length\":self.sequence_length,\n",
    "                       \"input_dim\":self.input_dim\n",
    "                   })\n",
    "            return config\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4956ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self,embed_size,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_size = embed_size\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads= num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_size)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_size)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim,activation=\"relu\"),layers.Dense(embed_size),])\n",
    "        self.layernorm_1= layers.LayerNormalization()\n",
    "        self.layernorm_2= layers.LayerNormalization()\n",
    "        self.layernorm_3= layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_size\": self.embed_size,\n",
    "            \"num_heads\":  self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def get_causal_attention_mask(self,inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size,sequence_length = input_shape[0],input_shape[1]\n",
    "        i = tf.range(sequence_length)[:,tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask= tf.cast(i >= j , dtype=\"int32\")\n",
    "        mask = tf.reshape(mask,(1,input_shape[1],input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size,-1),\n",
    "                   tf.constant([1,1],dtype=tf.int32)],axis=0)\n",
    "        return tf.tile(mask,mult)\n",
    "    def call(self,inputs,encoder_outputs,mask=None):\n",
    "        casual_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "            mask[:,tf.newaxis,:],dtype=\"int32\")\n",
    "            padding_mask= tf.minimum(padding_mask,casual_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "        query=inputs,value=inputs,key=inputs,attention_mask=casual_mask)\n",
    "        attention_output_1= self.layernorm_1(inputs+attention_output_1)\n",
    "        attention_output_2= self.attention_2(\n",
    "        query=attention_output_1,value=encoder_outputs,key=encoder_outputs,attention_mask=padding_mask)\n",
    "        attention_output_2=self.layernorm_2(attention_output_1+attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b594ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "vocab_size = 15000\n",
    "sequence_length=20\n",
    "encoder_inputs = keras.Input(shape=(None,),dtype=\"int64\",name=\"english\")\n",
    "x=PositionalEmbedding(sequence_length,vocab_size,embed_size)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_size,dense_dim,num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,),dtype=\"int64\",name=\"japanese\")\n",
    "x=PositionalEmbedding(sequence_length,vocab_size,embed_size)(decoder_inputs)\n",
    "x=TransformerDecoder(embed_size,dense_dim,num_heads)(x,encoder_outputs)\n",
    "x= layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size,activation=\"softmax\")(x)\n",
    "transformer= keras.Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75415331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " japanese (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   3845120     ['english[0][0]']                \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " positional_embedding_1 (Positi  (None, None, 256)   3845120     ['japanese[0][0]']               \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   5259520     ['positional_embedding_1[0][0]', \n",
      " erDecoder)                                                       'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, None, 15000)  3855000     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,960,216\n",
      "Trainable params: 19,960,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a6a96eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1022/1022 [==============================] - 966s 942ms/step - loss: 2.5676 - accuracy: 0.5175 - val_loss: 1.9918 - val_accuracy: 0.5969\n",
      "Epoch 2/40\n",
      "1022/1022 [==============================] - 982s 961ms/step - loss: 1.9035 - accuracy: 0.6158 - val_loss: 1.6302 - val_accuracy: 0.6583\n",
      "Epoch 3/40\n",
      "1022/1022 [==============================] - 984s 963ms/step - loss: 1.6376 - accuracy: 0.6610 - val_loss: 1.4922 - val_accuracy: 0.6851\n",
      "Epoch 4/40\n",
      "1022/1022 [==============================] - 992s 971ms/step - loss: 1.4742 - accuracy: 0.6893 - val_loss: 1.4152 - val_accuracy: 0.7006\n",
      "Epoch 5/40\n",
      "1022/1022 [==============================] - 986s 965ms/step - loss: 1.3559 - accuracy: 0.7102 - val_loss: 1.3800 - val_accuracy: 0.7093\n",
      "Epoch 6/40\n",
      "1022/1022 [==============================] - 984s 963ms/step - loss: 1.2607 - accuracy: 0.7270 - val_loss: 1.3507 - val_accuracy: 0.7181\n",
      "Epoch 7/40\n",
      "1022/1022 [==============================] - 999s 978ms/step - loss: 1.1859 - accuracy: 0.7408 - val_loss: 1.3365 - val_accuracy: 0.7235\n",
      "Epoch 8/40\n",
      "1022/1022 [==============================] - 995s 974ms/step - loss: 1.1162 - accuracy: 0.7535 - val_loss: 1.3529 - val_accuracy: 0.7252\n",
      "Epoch 9/40\n",
      "1022/1022 [==============================] - 991s 969ms/step - loss: 1.0575 - accuracy: 0.7649 - val_loss: 1.3619 - val_accuracy: 0.7274\n",
      "Epoch 10/40\n",
      "1022/1022 [==============================] - 978s 957ms/step - loss: 1.0055 - accuracy: 0.7748 - val_loss: 1.3519 - val_accuracy: 0.7300\n",
      "Epoch 11/40\n",
      "1022/1022 [==============================] - 975s 954ms/step - loss: 0.9553 - accuracy: 0.7836 - val_loss: 1.3685 - val_accuracy: 0.7325\n",
      "Epoch 12/40\n",
      "1022/1022 [==============================] - 977s 956ms/step - loss: 0.9095 - accuracy: 0.7934 - val_loss: 1.3553 - val_accuracy: 0.7365\n",
      "Epoch 13/40\n",
      "1022/1022 [==============================] - 992s 971ms/step - loss: 0.8682 - accuracy: 0.8012 - val_loss: 1.3701 - val_accuracy: 0.7367\n",
      "Epoch 14/40\n",
      "1022/1022 [==============================] - 1124s 1s/step - loss: 0.8310 - accuracy: 0.8083 - val_loss: 1.3676 - val_accuracy: 0.7387\n",
      "Epoch 15/40\n",
      "1022/1022 [==============================] - 1180s 1s/step - loss: 0.7983 - accuracy: 0.8150 - val_loss: 1.3831 - val_accuracy: 0.7386\n",
      "Epoch 16/40\n",
      "1022/1022 [==============================] - 1137s 1s/step - loss: 0.7659 - accuracy: 0.8214 - val_loss: 1.3842 - val_accuracy: 0.7421\n",
      "Epoch 17/40\n",
      "1022/1022 [==============================] - 1223s 1s/step - loss: 0.7407 - accuracy: 0.8268 - val_loss: 1.3959 - val_accuracy: 0.7408\n",
      "Epoch 18/40\n",
      "1022/1022 [==============================] - 1283s 1s/step - loss: 0.7146 - accuracy: 0.8320 - val_loss: 1.4096 - val_accuracy: 0.7418\n",
      "Epoch 19/40\n",
      "1022/1022 [==============================] - 1154s 1s/step - loss: 0.6911 - accuracy: 0.8369 - val_loss: 1.4191 - val_accuracy: 0.7437\n",
      "Epoch 20/40\n",
      "1022/1022 [==============================] - 1095s 1s/step - loss: 0.6696 - accuracy: 0.8413 - val_loss: 1.4324 - val_accuracy: 0.7425\n",
      "Epoch 21/40\n",
      "1022/1022 [==============================] - 1057s 1s/step - loss: 0.6489 - accuracy: 0.8453 - val_loss: 1.4426 - val_accuracy: 0.7440\n",
      "Epoch 22/40\n",
      "1022/1022 [==============================] - 1053s 1s/step - loss: 0.6314 - accuracy: 0.8494 - val_loss: 1.4598 - val_accuracy: 0.7413\n",
      "Epoch 23/40\n",
      "1022/1022 [==============================] - 1036s 1s/step - loss: 0.6149 - accuracy: 0.8529 - val_loss: 1.4439 - val_accuracy: 0.7449\n",
      "Epoch 24/40\n",
      "1022/1022 [==============================] - 1044s 1s/step - loss: 0.5972 - accuracy: 0.8568 - val_loss: 1.4612 - val_accuracy: 0.7432\n",
      "Epoch 25/40\n",
      "1022/1022 [==============================] - 1052s 1s/step - loss: 0.5820 - accuracy: 0.8600 - val_loss: 1.4815 - val_accuracy: 0.7451\n",
      "Epoch 26/40\n",
      "1022/1022 [==============================] - 1069s 1s/step - loss: 0.5695 - accuracy: 0.8631 - val_loss: 1.4913 - val_accuracy: 0.7428\n",
      "Epoch 27/40\n",
      "1022/1022 [==============================] - 1037s 1s/step - loss: 0.5567 - accuracy: 0.8656 - val_loss: 1.5071 - val_accuracy: 0.7462\n",
      "Epoch 28/40\n",
      "1022/1022 [==============================] - 1047s 1s/step - loss: 0.5441 - accuracy: 0.8685 - val_loss: 1.4948 - val_accuracy: 0.7458\n",
      "Epoch 29/40\n",
      "1022/1022 [==============================] - 1064s 1s/step - loss: 0.5320 - accuracy: 0.8711 - val_loss: 1.5204 - val_accuracy: 0.7464\n",
      "Epoch 30/40\n",
      "1022/1022 [==============================] - 1043s 1s/step - loss: 0.5212 - accuracy: 0.8732 - val_loss: 1.5273 - val_accuracy: 0.7455\n",
      "Epoch 31/40\n",
      "1022/1022 [==============================] - 1087s 1s/step - loss: 0.5099 - accuracy: 0.8757 - val_loss: 1.5348 - val_accuracy: 0.7449\n",
      "Epoch 32/40\n",
      "1022/1022 [==============================] - 1047s 1s/step - loss: 0.5016 - accuracy: 0.8775 - val_loss: 1.5629 - val_accuracy: 0.7434\n",
      "Epoch 33/40\n",
      "1022/1022 [==============================] - 1058s 1s/step - loss: 0.4938 - accuracy: 0.8793 - val_loss: 1.5376 - val_accuracy: 0.7465\n",
      "Epoch 34/40\n",
      "1022/1022 [==============================] - 985s 964ms/step - loss: 0.4847 - accuracy: 0.8816 - val_loss: 1.5570 - val_accuracy: 0.7457\n",
      "Epoch 35/40\n",
      "1022/1022 [==============================] - 986s 965ms/step - loss: 0.4750 - accuracy: 0.8834 - val_loss: 1.5790 - val_accuracy: 0.7467\n",
      "Epoch 36/40\n",
      "1022/1022 [==============================] - 1023s 1s/step - loss: 0.4694 - accuracy: 0.8846 - val_loss: 1.5774 - val_accuracy: 0.7470\n",
      "Epoch 37/40\n",
      "1022/1022 [==============================] - 1138s 1s/step - loss: 0.4629 - accuracy: 0.8859 - val_loss: 1.5907 - val_accuracy: 0.7444\n",
      "Epoch 38/40\n",
      "1022/1022 [==============================] - 1136s 1s/step - loss: 0.4550 - accuracy: 0.8882 - val_loss: 1.5927 - val_accuracy: 0.7466\n",
      "Epoch 39/40\n",
      "1022/1022 [==============================] - 1108s 1s/step - loss: 0.4498 - accuracy: 0.8894 - val_loss: 1.6076 - val_accuracy: 0.7437\n",
      "Epoch 40/40\n",
      "1022/1022 [==============================] - 1070s 1s/step - loss: 0.4442 - accuracy: 0.8906 - val_loss: 1.5989 - val_accuracy: 0.7473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bace337a00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(optimizer=\"rmsprop\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds,epochs=40,validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97d12a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "He has learned to be patient\n",
      "starttart彼に彼彼でのはでをだはだなははだ\n",
      "-\n",
      "I have low blood pressure\n",
      "starttart血血血血さでではししさししだがだ\n",
      "-\n",
      "I dont mind your staying here\n",
      "starttartいだここでしししこしんしよしとし\n",
      "-\n",
      "Dont go near the water until you learn how to swim\n",
      "starttart私んな理のんまましまますでしいし\n",
      "-\n",
      "That road is too narrow for a car to drive on\n",
      "starttartそはあはのははで道でうで車でまで\n",
      "-\n",
      "Inhale and exhale\n",
      "starttartやしやとしししではしすしてししが\n",
      "-\n",
      "She comes from a good family\n",
      "starttart彼は彼はでなでで彼で女だよで女だ\n",
      "-\n",
      "He is working hard in order to pass the entrance examination\n",
      "starttart彼の彼のにのの試試だのの入をはい\n",
      "-\n",
      "Dont waste time\n",
      "starttart無は時もでしではさすしすすで間な\n",
      "-\n",
      "How old is this temple\n",
      "starttartこしこははははeこでなでてででで\n",
      "-\n",
      "This book is too difficult for me to read\n",
      "starttartこにこのをっじで私ででではさでで\n",
      "-\n",
      "The storm knocked out power\n",
      "starttart嵐は嵐のははのさささにしなさでし\n",
      "-\n",
      "I think Tom is a very kind man\n",
      "starttartトしトトにのさささだムだてだムだ\n",
      "-\n",
      "The goods were transported by ship\n",
      "starttart料品料品でがでがあししししさでさ\n",
      "-\n",
      "Tom told me to shut the gate\n",
      "starttartトトト理がにをを私をムししししし\n",
      "-\n",
      "Was Tom the one who broke the window\n",
      "starttartトのトのでをさをトしムさ人eムだ\n",
      "-\n",
      "If I had time I would study French\n",
      "starttart僕しいもでしはでもしはしてしたし\n",
      "-\n",
      "I got up earlier than usual to get the first train\n",
      "starttart始は私ににににに電にめにいまはだ\n",
      "-\n",
      "The house isnt occupied now\n",
      "starttart家を家はでしししししなししししし\n",
      "-\n",
      "I think its a fox not a dog\n",
      "starttartキつチつでねじでしでしじもだしだ\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "jp_vocab= target_vectorization.get_vocabulary()\n",
    "jp_index_lookup = dict(zip(range(len(jp_vocab)),jp_vocab))\n",
    "max_decoded = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:,:-1]\n",
    "        predictions = transformer([tokenized_input_sentence,tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0,i,:])\n",
    "        sampled_token = jp_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return re.sub('\\W+' , \"\",decoded_sentence)\n",
    "test_eng_texts= [pair[0] for pair in test_pairs]\n",
    "for _ in range (20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "568c816e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 't',\n",
       " 'a',\n",
       " 'd',\n",
       " 'e',\n",
       " 'n',\n",
       " 's',\n",
       " 'r',\n",
       " 'い',\n",
       " 'は',\n",
       " 'た',\n",
       " 'の',\n",
       " 'な',\n",
       " 'て',\n",
       " 'に',\n",
       " 'っ',\n",
       " 'し',\n",
       " 'を',\n",
       " 'で',\n",
       " 'か',\n",
       " 'が',\n",
       " 'る',\n",
       " 'す',\n",
       " 'と',\n",
       " 'だ',\n",
       " 'ま',\n",
       " 'ん',\n",
       " 'こ',\n",
       " 'ト',\n",
       " 'よ',\n",
       " 'う',\n",
       " 'く',\n",
       " 'れ',\n",
       " 'ム',\n",
       " 'も',\n",
       " '彼',\n",
       " 'ら',\n",
       " 'り',\n",
       " 'あ',\n",
       " 'き',\n",
       " '私',\n",
       " 'ー',\n",
       " 'そ',\n",
       " 'さ',\n",
       " 'け',\n",
       " 'ち',\n",
       " 'ス',\n",
       " 'ン',\n",
       " 'お',\n",
       " 'ど',\n",
       " '日',\n",
       " 'つ',\n",
       " 'せ',\n",
       " '女',\n",
       " 'え',\n",
       " '行',\n",
       " 'ア',\n",
       " 'リ',\n",
       " '人',\n",
       " '何',\n",
       " 'め',\n",
       " 'わ',\n",
       " '見',\n",
       " '一',\n",
       " 'ラ',\n",
       " 'み',\n",
       " 'ゃ',\n",
       " 'ね',\n",
       " 'メ',\n",
       " '時',\n",
       " '今',\n",
       " '本',\n",
       " 'や',\n",
       " 'ろ',\n",
       " '思',\n",
       " '分',\n",
       " '言',\n",
       " '話',\n",
       " 'じ',\n",
       " '手',\n",
       " '事',\n",
       " '出',\n",
       " 'ば',\n",
       " '間',\n",
       " '気',\n",
       " 'べ',\n",
       " '食',\n",
       " '好',\n",
       " 'ょ',\n",
       " 'ル',\n",
       " '大',\n",
       " 'イ',\n",
       " '語',\n",
       " '来',\n",
       " '子',\n",
       " 'フ',\n",
       " 'ッ',\n",
       " '家',\n",
       " '車',\n",
       " 'ご',\n",
       " '前',\n",
       " '生',\n",
       " 'ク',\n",
       " '中',\n",
       " '知',\n",
       " '１',\n",
       " '自',\n",
       " '年',\n",
       " 'ド',\n",
       " 'テ',\n",
       " '学',\n",
       " '誰',\n",
       " '上',\n",
       " '会',\n",
       " '僕',\n",
       " 'レ',\n",
       " '方',\n",
       " '明',\n",
       " '君',\n",
       " 'ほ',\n",
       " '入',\n",
       " 'カ',\n",
       " 'コ',\n",
       " 'タ',\n",
       " 'バ',\n",
       " '電',\n",
       " '当',\n",
       " '持',\n",
       " '物',\n",
       " 'ぐ',\n",
       " 'ボ',\n",
       " 'げ',\n",
       " '聞',\n",
       " 'ず',\n",
       " '合',\n",
       " '買',\n",
       " 'パ',\n",
       " '０',\n",
       " '金',\n",
       " '仕',\n",
       " '部',\n",
       " '達',\n",
       " 'ジ',\n",
       " '下',\n",
       " 'チ',\n",
       " 'へ',\n",
       " '書',\n",
       " '全',\n",
       " 'キ',\n",
       " 'ぎ',\n",
       " '少',\n",
       " '３',\n",
       " '理',\n",
       " '目',\n",
       " '屋',\n",
       " '着',\n",
       " '昨',\n",
       " '父',\n",
       " '要',\n",
       " '待',\n",
       " '後',\n",
       " 'ビ',\n",
       " '２',\n",
       " 'オ',\n",
       " 'シ',\n",
       " '教',\n",
       " '校',\n",
       " '月',\n",
       " 'プ',\n",
       " '起',\n",
       " '外',\n",
       " '雨',\n",
       " '問',\n",
       " '意',\n",
       " '必',\n",
       " '度',\n",
       " '母',\n",
       " '強',\n",
       " '者',\n",
       " '新',\n",
       " '通',\n",
       " '夜',\n",
       " '読',\n",
       " '朝',\n",
       " '楽',\n",
       " '題',\n",
       " '住',\n",
       " '遅',\n",
       " '飲',\n",
       " 'ロ',\n",
       " '週',\n",
       " '帰',\n",
       " '切',\n",
       " '番',\n",
       " '取',\n",
       " '降',\n",
       " '二',\n",
       " 'マ',\n",
       " '名',\n",
       " '友',\n",
       " '歩',\n",
       " '高',\n",
       " '悪',\n",
       " '違',\n",
       " '親',\n",
       " '伝',\n",
       " '心',\n",
       " '供',\n",
       " 'ャ',\n",
       " '寝',\n",
       " '転',\n",
       " 'び',\n",
       " '立',\n",
       " 'ニ',\n",
       " '多',\n",
       " 'む',\n",
       " 'ケ',\n",
       " 'サ',\n",
       " '近',\n",
       " '先',\n",
       " '画',\n",
       " '水',\n",
       " '結',\n",
       " 'ィ',\n",
       " '最',\n",
       " '早',\n",
       " 'ひ',\n",
       " 'ブ',\n",
       " '開',\n",
       " '小',\n",
       " '無',\n",
       " '信',\n",
       " '考',\n",
       " 'ぶ',\n",
       " '終',\n",
       " 'ツ',\n",
       " '所',\n",
       " '味',\n",
       " '変',\n",
       " '使',\n",
       " 'ピ',\n",
       " '犬',\n",
       " '始',\n",
       " '勉',\n",
       " 'グ',\n",
       " '々',\n",
       " '緒',\n",
       " '国',\n",
       " '動',\n",
       " '発',\n",
       " '作',\n",
       " '同',\n",
       " '風',\n",
       " '長',\n",
       " '休',\n",
       " '良',\n",
       " '嫌',\n",
       " '毎',\n",
       " '真',\n",
       " '場',\n",
       " '音',\n",
       " '欲',\n",
       " '婚',\n",
       " '決',\n",
       " '乗',\n",
       " '運',\n",
       " '男',\n",
       " '俺',\n",
       " '以',\n",
       " 'ダ',\n",
       " '空',\n",
       " 'ヒ',\n",
       " '痛',\n",
       " '地',\n",
       " '泳',\n",
       " '死',\n",
       " '英',\n",
       " '紙',\n",
       " '計',\n",
       " '５',\n",
       " '解',\n",
       " '回',\n",
       " '曜',\n",
       " '我',\n",
       " '成',\n",
       " '用',\n",
       " '機',\n",
       " '実',\n",
       " '試',\n",
       " '説',\n",
       " '面',\n",
       " '過',\n",
       " '体',\n",
       " '料',\n",
       " '道',\n",
       " '的',\n",
       " '旅',\n",
       " '予',\n",
       " '忘',\n",
       " '付',\n",
       " '不',\n",
       " 'ュ',\n",
       " '願',\n",
       " '約',\n",
       " '足',\n",
       " '歳',\n",
       " '止',\n",
       " '口',\n",
       " 'ざ',\n",
       " '重',\n",
       " '落',\n",
       " '眠',\n",
       " '座',\n",
       " 'ぞ',\n",
       " '性',\n",
       " '覚',\n",
       " 'ナ',\n",
       " '病',\n",
       " '働',\n",
       " 'ペ',\n",
       " '駅',\n",
       " '正',\n",
       " '定',\n",
       " '数',\n",
       " '映',\n",
       " 'ワ',\n",
       " 'ベ',\n",
       " '白',\n",
       " '戻',\n",
       " 'ポ',\n",
       " '失',\n",
       " '世',\n",
       " 'デ',\n",
       " 'ガ',\n",
       " '感',\n",
       " 'ズ',\n",
       " '引',\n",
       " '難',\n",
       " '歌',\n",
       " '文',\n",
       " '赤',\n",
       " '他',\n",
       " '両',\n",
       " '飛',\n",
       " 'ホ',\n",
       " '急',\n",
       " '天',\n",
       " '雪',\n",
       " '業',\n",
       " '訳',\n",
       " '然',\n",
       " '故',\n",
       " '0',\n",
       " '力',\n",
       " '頭',\n",
       " '美',\n",
       " '猫',\n",
       " 'ウ',\n",
       " 'ミ',\n",
       " '込',\n",
       " '受',\n",
       " '初',\n",
       " '身',\n",
       " '質',\n",
       " 'ゴ',\n",
       " '対',\n",
       " 'ョ',\n",
       " 'ギ',\n",
       " '1',\n",
       " '助',\n",
       " '腹',\n",
       " '忙',\n",
       " '得',\n",
       " '店',\n",
       " '医',\n",
       " 'ネ',\n",
       " '返',\n",
       " '写',\n",
       " '配',\n",
       " 'ぱ',\n",
       " '残',\n",
       " '幸',\n",
       " '夕',\n",
       " '閉',\n",
       " '窓',\n",
       " '山',\n",
       " 'ハ',\n",
       " 'エ',\n",
       " '頼',\n",
       " '席',\n",
       " '昼',\n",
       " '愛',\n",
       " '向',\n",
       " '代',\n",
       " '笑',\n",
       " '海',\n",
       " '次',\n",
       " '続',\n",
       " '直',\n",
       " '寒',\n",
       " 'ぜ',\n",
       " 'ソ',\n",
       " '息',\n",
       " '６',\n",
       " '邪',\n",
       " '験',\n",
       " '木',\n",
       " '連',\n",
       " '疲',\n",
       " '3',\n",
       " '十',\n",
       " 'ぼ',\n",
       " '茶',\n",
       " '消',\n",
       " '送',\n",
       " '若',\n",
       " '兄',\n",
       " '段',\n",
       " '午',\n",
       " '洗',\n",
       " '走',\n",
       " '関',\n",
       " '怒',\n",
       " '単',\n",
       " '花',\n",
       " '勝',\n",
       " '泣',\n",
       " '飯',\n",
       " '９',\n",
       " '離',\n",
       " '議',\n",
       " '晩',\n",
       " '夢',\n",
       " 'ノ',\n",
       " 'セ',\n",
       " '調',\n",
       " '冷',\n",
       " '公',\n",
       " '宿',\n",
       " '声',\n",
       " '確',\n",
       " '社',\n",
       " '弟',\n",
       " '内',\n",
       " 'づ',\n",
       " '靴',\n",
       " '夫',\n",
       " '列',\n",
       " '元',\n",
       " '顔',\n",
       " '色',\n",
       " '置',\n",
       " '三',\n",
       " '野',\n",
       " '界',\n",
       " '夏',\n",
       " '警',\n",
       " '張',\n",
       " '士',\n",
       " '選',\n",
       " '探',\n",
       " '頃',\n",
       " '園',\n",
       " '遊',\n",
       " '答',\n",
       " '散',\n",
       " '払',\n",
       " '安',\n",
       " '習',\n",
       " '杯',\n",
       " '売',\n",
       " '嘘',\n",
       " '半',\n",
       " '由',\n",
       " '別',\n",
       " 'ゆ',\n",
       " '葉',\n",
       " '能',\n",
       " '怖',\n",
       " '図',\n",
       " '４',\n",
       " '火',\n",
       " '服',\n",
       " '遠',\n",
       " '束',\n",
       " '魚',\n",
       " '館',\n",
       " '背',\n",
       " '現',\n",
       " '有',\n",
       " '員',\n",
       " '速',\n",
       " 'ェ',\n",
       " '院',\n",
       " '球',\n",
       " '2',\n",
       " '素',\n",
       " '川',\n",
       " 'ぁ',\n",
       " '在',\n",
       " '呼',\n",
       " 'ふ',\n",
       " '記',\n",
       " '申',\n",
       " '命',\n",
       " '借',\n",
       " '駄',\n",
       " '貸',\n",
       " '注',\n",
       " '法',\n",
       " '太',\n",
       " '備',\n",
       " '盗',\n",
       " '暑',\n",
       " '可',\n",
       " '危',\n",
       " '談',\n",
       " '期',\n",
       " '族',\n",
       " '苦',\n",
       " '果',\n",
       " '殺',\n",
       " '辞',\n",
       " '薬',\n",
       " '暗',\n",
       " '限',\n",
       " '守',\n",
       " '鍵',\n",
       " '経',\n",
       " '煙',\n",
       " '平',\n",
       " '去',\n",
       " '髪',\n",
       " 'モ',\n",
       " '謝',\n",
       " '暮',\n",
       " '古',\n",
       " 'ザ',\n",
       " '８',\n",
       " '案',\n",
       " '放',\n",
       " '値',\n",
       " '満',\n",
       " '７',\n",
       " '相',\n",
       " '卵',\n",
       " '鳴',\n",
       " '絶',\n",
       " '誕',\n",
       " '点',\n",
       " '察',\n",
       " '吸',\n",
       " '便',\n",
       " '似',\n",
       " '戦',\n",
       " '常',\n",
       " '告',\n",
       " '台',\n",
       " '刻',\n",
       " 'ぬ',\n",
       " '馬',\n",
       " '慣',\n",
       " '活',\n",
       " '傘',\n",
       " '京',\n",
       " '熱',\n",
       " '渡',\n",
       " '東',\n",
       " '功',\n",
       " '丈',\n",
       " '恐',\n",
       " '徒',\n",
       " '町',\n",
       " '準',\n",
       " '格',\n",
       " '捕',\n",
       " '万',\n",
       " '断',\n",
       " '師',\n",
       " '報',\n",
       " '号',\n",
       " '価',\n",
       " '姉',\n",
       " '交',\n",
       " '認',\n",
       " '様',\n",
       " '静',\n",
       " '突',\n",
       " '末',\n",
       " '晴',\n",
       " '指',\n",
       " '役',\n",
       " '辺',\n",
       " '興',\n",
       " '授',\n",
       " '建',\n",
       " '優',\n",
       " '任',\n",
       " '絵',\n",
       " '流',\n",
       " '集',\n",
       " '箱',\n",
       " '利',\n",
       " '驚',\n",
       " '青',\n",
       " '許',\n",
       " '育',\n",
       " '礼',\n",
       " '留',\n",
       " '普',\n",
       " '昔',\n",
       " '都',\n",
       " '敵',\n",
       " '老',\n",
       " '宅',\n",
       " '簡',\n",
       " '牛',\n",
       " '帽',\n",
       " 'ァ',\n",
       " '階',\n",
       " '険',\n",
       " '肉',\n",
       " '望',\n",
       " '暇',\n",
       " '寄',\n",
       " '室',\n",
       " '件',\n",
       " '情',\n",
       " '弾',\n",
       " '娘',\n",
       " '妹',\n",
       " '務',\n",
       " '健',\n",
       " '非',\n",
       " '酒',\n",
       " '想',\n",
       " '市',\n",
       " 'ゼ',\n",
       " '冗',\n",
       " '割',\n",
       " '焼',\n",
       " '提',\n",
       " '品',\n",
       " '類',\n",
       " '職',\n",
       " '念',\n",
       " '客',\n",
       " '字',\n",
       " '反',\n",
       " '腕',\n",
       " '絡',\n",
       " '登',\n",
       " '済',\n",
       " '治',\n",
       " '求',\n",
       " '恋',\n",
       " '康',\n",
       " '亡',\n",
       " '観',\n",
       " '逃',\n",
       " '訪',\n",
       " '状',\n",
       " '滞',\n",
       " '支',\n",
       " '加',\n",
       " '係',\n",
       " '鳥',\n",
       " '禁',\n",
       " '布',\n",
       " '迷',\n",
       " '財',\n",
       " '敗',\n",
       " '具',\n",
       " '進',\n",
       " '深',\n",
       " '歯',\n",
       " '机',\n",
       " '困',\n",
       " 'ヨ',\n",
       " '銀',\n",
       " '耳',\n",
       " '種',\n",
       " '主',\n",
       " '独',\n",
       " '居',\n",
       " '冬',\n",
       " '円',\n",
       " '負',\n",
       " '局',\n",
       " '喜',\n",
       " '到',\n",
       " '飼',\n",
       " '除',\n",
       " '血',\n",
       " '富',\n",
       " '泊',\n",
       " '乳',\n",
       " '曲',\n",
       " '庭',\n",
       " '参',\n",
       " '隣',\n",
       " '越',\n",
       " '賛',\n",
       " '星',\n",
       " '掛',\n",
       " '抜',\n",
       " '壊',\n",
       " '倒',\n",
       " '争',\n",
       " 'ヤ',\n",
       " '黒',\n",
       " '退',\n",
       " '石',\n",
       " '庫',\n",
       " '密',\n",
       " '修',\n",
       " '酔',\n",
       " '途',\n",
       " '構',\n",
       " '妻',\n",
       " '化',\n",
       " '傷',\n",
       " '黙',\n",
       " '論',\n",
       " '砂',\n",
       " '容',\n",
       " '右',\n",
       " '隠',\n",
       " '筆',\n",
       " '慢',\n",
       " '悲',\n",
       " '左',\n",
       " '個',\n",
       " '護',\n",
       " '第',\n",
       " '祖',\n",
       " '片',\n",
       " 'ゲ',\n",
       " '翻',\n",
       " '替',\n",
       " '振',\n",
       " '余',\n",
       " '脱',\n",
       " '撮',\n",
       " '完',\n",
       " '原',\n",
       " '際',\n",
       " '陽',\n",
       " '船',\n",
       " '減',\n",
       " '欠',\n",
       " '押',\n",
       " '抱',\n",
       " '折',\n",
       " '投',\n",
       " '広',\n",
       " '草',\n",
       " '招',\n",
       " '土',\n",
       " '表',\n",
       " '秘',\n",
       " '科',\n",
       " '横',\n",
       " '嵐',\n",
       " '勤',\n",
       " '疑',\n",
       " '温',\n",
       " '春',\n",
       " '存',\n",
       " '叔',\n",
       " 'ヘ',\n",
       " '首',\n",
       " '頑',\n",
       " '路',\n",
       " '術',\n",
       " '給',\n",
       " '皆',\n",
       " '産',\n",
       " '態',\n",
       " '触',\n",
       " '和',\n",
       " '匹',\n",
       " '蔵',\n",
       " '紅',\n",
       " '暖',\n",
       " '床',\n",
       " 'ぽ',\n",
       " '演',\n",
       " '嬉',\n",
       " '仲',\n",
       " '釣',\n",
       " '破',\n",
       " '短',\n",
       " '湖',\n",
       " '政',\n",
       " '撃',\n",
       " '齢',\n",
       " '鹿',\n",
       " '差',\n",
       " '裏',\n",
       " '罪',\n",
       " '打',\n",
       " '光',\n",
       " '側',\n",
       " '郵',\n",
       " '追',\n",
       " '港',\n",
       " '掃',\n",
       " '懸',\n",
       " '弁',\n",
       " '専',\n",
       " '塩',\n",
       " '呂',\n",
       " '冊',\n",
       " '鉛',\n",
       " '迎',\n",
       " '神',\n",
       " '婦',\n",
       " '泥',\n",
       " '久',\n",
       " '雑',\n",
       " '責',\n",
       " '証',\n",
       " '義',\n",
       " '戸',\n",
       " '収',\n",
       " '詩',\n",
       " '製',\n",
       " '特',\n",
       " '判',\n",
       " '震',\n",
       " '聴',\n",
       " '届',\n",
       " '労',\n",
       " '騒',\n",
       " '田',\n",
       " '根',\n",
       " '怪',\n",
       " '坊',\n",
       " '吹',\n",
       " '停',\n",
       " '組',\n",
       " '犯',\n",
       " '池',\n",
       " '椅',\n",
       " '枚',\n",
       " '扱',\n",
       " '才',\n",
       " '官',\n",
       " '器',\n",
       " '例',\n",
       " '互',\n",
       " '与',\n",
       " '鏡',\n",
       " '誤',\n",
       " '浴',\n",
       " '村',\n",
       " '捨',\n",
       " '応',\n",
       " '式',\n",
       " '則',\n",
       " '低',\n",
       " '頂',\n",
       " '銃',\n",
       " '趣',\n",
       " '賢',\n",
       " '裕',\n",
       " '荷',\n",
       " '混',\n",
       " '毛',\n",
       " '未',\n",
       " '慮',\n",
       " '延',\n",
       " '帯',\n",
       " '努',\n",
       " 'ヶ',\n",
       " '皿',\n",
       " '激',\n",
       " '従',\n",
       " '魔',\n",
       " '費',\n",
       " '規',\n",
       " '沈',\n",
       " '査',\n",
       " '害',\n",
       " '卒',\n",
       " '効',\n",
       " '制',\n",
       " '陸',\n",
       " '課',\n",
       " '詳',\n",
       " '緊',\n",
       " '糖',\n",
       " '況',\n",
       " '免',\n",
       " '超',\n",
       " '舞',\n",
       " '舎',\n",
       " '線',\n",
       " '符',\n",
       " '橋',\n",
       " '描',\n",
       " '惑',\n",
       " '工',\n",
       " '姿',\n",
       " '喋',\n",
       " '保',\n",
       " '5',\n",
       " '識',\n",
       " '視',\n",
       " '涙',\n",
       " '携',\n",
       " '接',\n",
       " '奥',\n",
       " '壁',\n",
       " '塗',\n",
       " '周',\n",
       " '勘',\n",
       " '処',\n",
       " '等',\n",
       " '氏',\n",
       " '森',\n",
       " '攻',\n",
       " '尽',\n",
       " '障',\n",
       " '門',\n",
       " '燃',\n",
       " '濯',\n",
       " '油',\n",
       " '易',\n",
       " '忠',\n",
       " '因',\n",
       " '介',\n",
       " '6',\n",
       " '街',\n",
       " '菜',\n",
       " '島',\n",
       " '尋',\n",
       " '喫',\n",
       " '司',\n",
       " '六',\n",
       " '八',\n",
       " '候',\n",
       " '五',\n",
       " '装',\n",
       " '穴',\n",
       " '検',\n",
       " '奴',\n",
       " '型',\n",
       " '偶',\n",
       " '骨',\n",
       " '更',\n",
       " '悔',\n",
       " '形',\n",
       " '履',\n",
       " '米',\n",
       " '甘',\n",
       " '毒',\n",
       " '歴',\n",
       " '棒',\n",
       " '復',\n",
       " '召',\n",
       " '勢',\n",
       " 'ゅ',\n",
       " '貴',\n",
       " '誘',\n",
       " '虫',\n",
       " '算',\n",
       " '瓶',\n",
       " '汚',\n",
       " '民',\n",
       " '捜',\n",
       " '増',\n",
       " '儀',\n",
       " '位',\n",
       " '伺',\n",
       " '駐',\n",
       " '福',\n",
       " '王',\n",
       " '浮',\n",
       " '植',\n",
       " '奇',\n",
       " '史',\n",
       " '量',\n",
       " '眼',\n",
       " '棚',\n",
       " '千',\n",
       " '再',\n",
       " '像',\n",
       " '飽',\n",
       " '霊',\n",
       " '辛',\n",
       " '象',\n",
       " '細',\n",
       " '節',\n",
       " '巻',\n",
       " '四',\n",
       " '営',\n",
       " '雲',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d85076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6ac0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
